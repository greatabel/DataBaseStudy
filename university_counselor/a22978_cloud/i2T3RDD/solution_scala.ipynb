{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da49953a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://10.248.33.227:4040\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1665713687317)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "shakes: org.apache.spark.rdd.RDD[String] = nbs/shakespeare.txt MapPartitionsRDD[1] at textFile at <console>:25\n",
       "verbs: Array[String] = Array(abash, abashed, abashed, abashes, abashing, abate, abated, abated, abates, abating, abide, abode, abode, abides, abiding, absorb, absorbed, absorbed, absorbs, absorbing, accept, accepted, accepted, accepts, accepting, accompany, accompanied, accompanied, accompanies, accompanying, ache, ached, ached, aches, aching, achieve, achieved, achieved, achieves, achieving, acquire, acquired, acquired, acquires, acquiring, act, acted, acted, acts, acting, add, added, added, adds, adding, address, addressed, addressed, addresses, addressing, adjust, adjusted, adjusted, adjusts, adjusting, admire, admired, admired, admires, admiring, admit, admitted, admitted, ...\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* q1 */\n",
    "val shakes = sc.textFile(\"nbs/shakespeare.txt\")\n",
    "val verbs = sc.textFile(\"nbs/all_verbs.txt\").collect()\n",
    "val verb_dict = sc.textFile(\"nbs/verb_dict.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ea3cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "remove_empty_lines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at filter at <console>:25\n",
       "remove_punctuation: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[7] at map at <console>:26\n",
       "my_cap: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[8] at map at <console>:27\n",
       "map: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at flatMap at <console>:29\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* q2 */\n",
    "val remove_empty_lines = shakes.filter(row => !row.isEmpty)\n",
    "val remove_punctuation = remove_empty_lines.map(_.replaceAll(\"[,.!?:;]\", \"\").trim)\n",
    "val my_cap = remove_punctuation.map(x=>x.toLowerCase)\n",
    "\n",
    "val map = my_cap.flatMap(line=> line.split(\" \"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a9dd1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.InternalError",
     "evalue": " java.lang.IllegalAccessException: final field has no write access: $Lambda$2754/0x0000000801743260.arg$1/putField, from class java.lang.Object (module java.base)",
     "output_type": "error",
     "traceback": [
      "java.lang.InternalError: java.lang.IllegalAccessException: final field has no write access: $Lambda$2754/0x0000000801743260.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:167)",
      "  at java.base/jdk.internal.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:176)",
      "  at java.base/java.lang.reflect.Field.acquireOverrideFieldAccessor(Field.java:1184)",
      "  at java.base/java.lang.reflect.Field.getOverrideFieldAccessor(Field.java:1153)",
      "  at java.base/java.lang.reflect.Field.set(Field.java:820)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:406)",
      "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)",
      "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2491)",
      "  at org.apache.spark.rdd.RDD.$anonfun$filter$1(RDD.scala:431)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.filter(RDD.scala:430)",
      "  ... 34 elided",
      "Caused by: java.lang.IllegalAccessException: final field has no write access: $Lambda$2754/0x0000000801743260.arg$1/putField, from class java.lang.Object (module java.base)",
      "  at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectField(MethodHandles.java:3494)",
      "  at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectSetter(MethodHandles.java:3485)",
      "  at java.base/java.lang.invoke.MethodHandleImpl$1.unreflectField(MethodHandleImpl.java:1637)",
      "  at java.base/jdk.internal.reflect.MethodHandleAccessorFactory.newFieldAccessor(MethodHandleAccessorFactory.java:145)",
      "  ... 46 more",
      ""
     ]
    }
   ],
   "source": [
    "/* q3 */\n",
    "val match_verbs = map.filter(x=> verbs.contains(x))\n",
    "\n",
    "val count_verbs = match_verbs.map(x=>(x,1)).reduceByKey((a,b)=>a+b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* q4 */\n",
    "val verbslist = verb_dict.map(s=>s.split(\",\").map(_.trim)).collect\n",
    "\n",
    "val verbMap = verbslist.flatMap(e => {\n",
    "    e.map(i=> i -> e(0))\n",
    "}).toMap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8b5ccb",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "25: error: not found: value verbMap",
     "output_type": "error",
     "traceback": [
      "<console>:25: error: not found: value verbMap",
      "       val bdVerbMap = sc.broadcast(verbMap)",
      "                                    ^",
      "<console>:26: error: not found: value count_verbs",
      "       val myTop = count_verbs.map(t=>(bdVerbMap.value.getOrElse(t._1, t._1), t._2)).reduceByKey((x,y)=>x+y)",
      "                   ^",
      ""
     ]
    }
   ],
   "source": [
    "/* q5 */\n",
    "val bdVerbMap = sc.broadcast(verbMap)\n",
    "val myTop = count_verbs.map(t=>(bdVerbMap.value.getOrElse(t._1, t._1), t._2)).reduceByKey((x,y)=>x+y)\n",
    "\n",
    "myTop.map(x=>x._2->x._1).sortByKey(false).map(x=>x._2->x._1).take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9502752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
